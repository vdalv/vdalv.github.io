---
layout: post
comments: false
title:  "GANfield: Something, Something GAT Pun"
date:   2018-12-04 11:14:00
---

### Introduction
This exercise was done just to see if it was possible to generate decent quality images from a small-ish (2,500 images) personal dataset. Over the past year, I'd seen some fairly lackluster results. However, when [Ian Goodfellow retweeted](https://twitter.com/goodfellow_ian/status/1015644004501712896) Lars Mescheder's [GAN_stability project](https://github.com/LMescheder/GAN_stability), I took one look at the results and knew the time was right.

[![](https://raw.githubusercontent.com/LMescheder/GAN_stability/master/results/celebA-HQ.jpg)](https://raw.githubusercontent.com/LMescheder/GAN_stability/master/results/celebA-HQ.jpg)
(Some of Lars' example results)

### Data Collection
I made a simple Python script to download most of the Garfield comic strip, excluding Sunday posts (due to their [different format](https://imgur.com/a/R2MyDOb) - if I ever need more data, I can always go back and get them) and newer posts (2014+, because I'm not a fan of the drawing style - though I admit, it's similar to some 2012 and 2013 posts). Then I split all of the images into three (one image per frame), which yielded 37,557 images. To keep things simpler for the GAN, I decided to include only images that featured Garfield and Garfield alone (background items were allowed, for the most part.)

To avoid sifting through all 37,000+ images myself, I setup a [simple classifier](https://www.youtube.com/watch?v=oXpsAiSajE0) which sorted the images between two classes (GarfieldOnly and NotGarfieldOnly). This approach worked alright, but it was still time-consuming to retrain the classifer after sorting its results. In the end, I was left with 2,500 "GarfieldOnly" images (see below for some examples).

[![](https://i.imgur.com/2RScOSw.jpg)](https://i.imgur.com/2RScOSw.jpg)
(Examples from the dataset - Note: This image (and all other dataset example images) was generated by GAN_stability, which flips some of the images, hence the backwards text)

### Training - Take 1
I edited the [CelebA config](https://github.com/LMescheder/GAN_stability/blob/master/configs/celebAHQ.yaml), and kicked off the training. Overall, I trained it for 1,500,000 iterations. Here's some of the results:

[![](https://i.imgur.com/kHOHMKQ.jpg)](https://i.imgur.com/kHOHMKQ.jpg)
(156,000 Iterations)

[![](https://i.imgur.com/t5sAVZs.jpg)](https://i.imgur.com/t5sAVZs.jpg)
(613,000 Iterations)

[![](https://i.imgur.com/Gj53DIO.jpg)](https://i.imgur.com/Gj53DIO.jpg)
(622,000 Iterations)

[![](https://i.imgur.com/dGgQ4OI.jpg)](https://i.imgur.com/dGgQ4OI.jpg)
(866,000 Iterations)

[![](https://i.imgur.com/XRyCtVi.jpg)](https://i.imgur.com/XRyCtVi.jpg)
(914,000 Iterations)

[![](https://i.imgur.com/DsO0iCz.jpg)](https://i.imgur.com/DsO0iCz.jpg)
(982,000 Iterations)

[![](https://i.imgur.com/0KtczQX.jpg)](https://i.imgur.com/0KtczQX.jpg)
(985,000 Iterations)

[![](https://i.imgur.com/Q4XD0OF.jpg)](https://i.imgur.com/Q4XD0OF.jpg)
(993,000 Iterations)

[![](https://i.imgur.com/IANQUwn.jpg)](https://i.imgur.com/IANQUwn.jpg)
(995,000 Iterations)

[![](https://i.imgur.com/NY9oqfY.jpg)](https://i.imgur.com/NY9oqfY.jpg)
(1,141,000 Iterations)

[![](https://i.imgur.com/9BZzFDR.jpg)](https://i.imgur.com/9BZzFDR.jpg)
(1,195,000 Iterations)

[![](https://i.imgur.com/ZnIXzAs.jpg)](https://i.imgur.com/ZnIXzAs.jpg)
(1,200,000 Iterations)

[![](https://i.imgur.com/wh8iaPr.jpg)](https://i.imgur.com/wh8iaPr.jpg)
(1,262,000 Iterations)

[![](https://i.imgur.com/rbgrD2S.jpg)](https://i.imgur.com/rbgrD2S.jpg)
(1,281,000 Iterations)

[![](https://i.imgur.com/AMwL5dn.jpg)](https://i.imgur.com/AMwL5dn.jpg)
(1,300,000 Iterations)

[![](https://i.imgur.com/7VOGS4I.jpg)](https://i.imgur.com/7VOGS4I.jpg)
(1,334,000 Iterations)

[![](https://i.imgur.com/6fNalkk.jpg)](https://i.imgur.com/6fNalkk.jpg)
(1,380,000 Iterations)

[![](https://i.imgur.com/42sGjwc.jpg)](https://i.imgur.com/42sGjwc.jpg)
(1,391,000 Iterations)

[![](https://i.imgur.com/ZRk0tPO.jpg)](https://i.imgur.com/ZRk0tPO.jpg)
(1,436,000 Iterations)

[![](https://i.imgur.com/3O3hm73.jpg)](https://i.imgur.com/3O3hm73.jpg)
(1,437,000 Iterations)

[![](https://i.imgur.com/6myOT9z.jpg)](https://i.imgur.com/6myOT9z.jpg)
(1,438,000 Iterations)

[![](https://i.imgur.com/leRpkyQ.jpg)](https://i.imgur.com/leRpkyQ.jpg)
(1,500,000 Iterations)

### Dataset 2
After 1,000,000 iterations of training with the original dataset, I decided that I could achieve better results by making the input images less diverse. So, I removed the older images (1991 and lower) due to their distinctly different styles (see examples below). Then, I separated the first dataset into three main categories: GarfieldStanding, GarfieldSitting, and Neither. The 'GarfieldStanding' and 'GarfieldSitting' categories were further divided into categories like 'standingOnScale', 'standingWithThings', and 'standingPlain'. This particular dataset is the 'standingPlain' subset of the 'GarfieldStanding' category (which was the largest, with 769 images).

[![](https://i.imgur.com/0QaM0JJ.jpg)](https://i.imgur.com/0QaM0JJ.jpg)
(Examples of the different Garfield drawing styles through the years)

[![](https://i.imgur.com/i2zwaAp.jpg)](https://i.imgur.com/i2zwaAp.jpg)
(Example images from the 2nd dataset, 'standingPlain')


### Training - Take 2
I pointed the config file to this dataset, and set it to train on my second machine. Here's some of the results:

[![](https://i.imgur.com/UUTmlZ9.jpg)](https://i.imgur.com/UUTmlZ9.jpg)
(228,000 Iterations)

[![](https://i.imgur.com/XrLEB1f.jpg)](https://i.imgur.com/XrLEB1f.jpg)
(263,000 Iterations)

[![](https://i.imgur.com/LCCU30u.jpg)](https://i.imgur.com/LCCU30u.jpg)
(328,000 Iterations)

[![](https://i.imgur.com/EObWGSl.jpg)](https://i.imgur.com/EObWGSl.jpg)
(375,000 Iterations)

[![](https://i.imgur.com/xYJi0bF.jpg)](https://i.imgur.com/xYJi0bF.jpg)
(390,000 Iterations)

[![](https://i.imgur.com/wX8B6ak.jpg)](https://i.imgur.com/wX8B6ak.jpg)
(416,000 Iterations)

[![](https://i.imgur.com/ZqSMlWW.jpg)](https://i.imgur.com/ZqSMlWW.jpg)
(435,000 Iterations)

[![](https://i.imgur.com/7OLbuOo.jpg)](https://i.imgur.com/7OLbuOo.jpg)
(462,000 Iterations)

[![](https://i.imgur.com/JhC281a.jpg)](https://i.imgur.com/JhC281a.jpg)
(465,000 Iterations)

[![](https://i.imgur.com/dLGi6bR.jpg)](https://i.imgur.com/dLGi6bR.jpg)
(466,000 Iterations)

[![](https://i.imgur.com/rHYckeM.jpg)](https://i.imgur.com/rHYckeM.jpg)
(469,000 Iterations)

[![](https://i.imgur.com/Ft80Fu6.jpg)](https://i.imgur.com/Ft80Fu6.jpg)
(477,000 Iterations)

[![](https://i.imgur.com/jOb2tlx.jpg)](https://i.imgur.com/jOb2tlx.jpg)
(481,000 Iterations)

[![](https://i.imgur.com/9DwFob9.jpg)](https://i.imgur.com/9DwFob9.jpg)
(482,000 Iterations)

[![](https://i.imgur.com/jOb2tlx.jpg)](https://i.imgur.com/jOb2tlx.jpg)
(483,000 Iterations)

[![](https://i.imgur.com/FvE89CS.jpg)](https://i.imgur.com/FvE89CS.jpg)
(487,000 Iterations)

[![](https://i.imgur.com/wwtXjUk.jpg)](https://i.imgur.com/wwtXjUk.jpg)
(488,000 Iterations)

[![](https://i.imgur.com/CGxNUzM.jpg)](https://i.imgur.com/CGxNUzM.jpg)
(496,000 Iterations)

[![](https://i.imgur.com/LKMSLAT.jpg)](https://i.imgur.com/LKMSLAT.jpg)
(509,000 Iterations)

[![](https://i.imgur.com/k3pHCzM.jpg)](https://i.imgur.com/k3pHCzM.jpg)
(510,000 Iterations)

[![](https://i.imgur.com/VqEix1o.jpg)](https://i.imgur.com/VqEix1o.jpg)
(513,000 Iterations)

[![](https://i.imgur.com/yfEWITu.jpg)](https://i.imgur.com/yfEWITu.jpg)
(520,000 Iterations)

[![](https://i.imgur.com/1hHK5v7.jpg)](https://i.imgur.com/1hHK5v7.jpg)
(521,000 Iterations)

[![](https://i.imgur.com/qdJ5IYo.jpg)](https://i.imgur.com/qdJ5IYo.jpg)
(522,000 Iterations)

[![](https://i.imgur.com/7JCtQz3.jpg)](https://i.imgur.com/7JCtQz3.jpg)
(527,000 Iterations)

[![](https://i.imgur.com/PzZ0Qsb.jpg)](https://i.imgur.com/PzZ0Qsb.jpg)
(530,000 Iterations)

[![](https://i.imgur.com/Ly7i3cW.jpg)](https://i.imgur.com/Ly7i3cW.jpg)
(533,000 Iterations)

[![](https://i.imgur.com/c6otz0R.jpg)](https://i.imgur.com/c6otz0R.jpg)
(547,000 Iterations)

### Dataset 3
This dataset is the 'sittingPlain' subset of the 'GarfieldSitting' category, mentioned in the Dataset 2 section. It contains only 313 images.

[![](https://i.imgur.com/Mxdmmuj.jpg)](https://i.imgur.com/Mxdmmuj.jpg)
(Example images from the 3rd dataset, 'sittingPlain')

### Training - Take 3
Once again, the config file was updated to point to the new dataset. This training was done on my main machine (now that Take 1 had finished 1,500,000 iterations).

[![](https://i.imgur.com/OHpgUot.jpg)](https://i.imgur.com/OHpgUot.jpg)
(196,000 Iterations)

[![](https://i.imgur.com/mxnRb1O.jpg)](https://i.imgur.com/mxnRb1O.jpg)
(209,000 Iterations)

[![](https://i.imgur.com/Kfm2BMa.jpg)](https://i.imgur.com/Kfm2BMa.jpg)
(214,000 Iterations)

[![](https://i.imgur.com/opKrqHb.jpg)](https://i.imgur.com/opKrqHb.jpg)
(229,000 Iterations)

[![](https://i.imgur.com/qDALCPP.jpg)](https://i.imgur.com/qDALCPP.jpg)
(238,000 Iterations)

[![](https://i.imgur.com/ZAcK2sl.jpg)](https://i.imgur.com/ZAcK2sl.jpg)
(259,000 Iterations)

[![](https://i.imgur.com/e9dMLIe.jpg)](https://i.imgur.com/e9dMLIe.jpg)
(275,000 Iterations)

[![](https://i.imgur.com/u0Fhyek.jpg)](https://i.imgur.com/u0Fhyek.jpg)
(280,000 Iterations)

[![](https://i.imgur.com/6ENnnp3.jpg)](https://i.imgur.com/6ENnnp3.jpg)
(281,000 Iterations)

### Results Overview
[![](https://i.imgur.com/p3CAHLA.jpg)](https://i.imgur.com/p3CAHLA.jpg)

### Why, man, why?
As I stated in the Introduction, this was done simply to see if I could get decent results with a fairly small dataset. Now, the reason I chose Garfield, of all things to generate, is because I thought the data collection process would be much simpler than what I really wanted to generate (Calvin and Hobbes). However, considering all of the Garfield classifier re-training and data sorting, I believe the Calvin dataset was much easier to build. That being said, I'll elaborate more in the next month or so, when I've generated some decent results with the dataset pictured below.

[![](https://i.imgur.com/rOyn0KP.jpg)](https://i.imgur.com/rOyn0KP.jpg)
(Examples from my Calvin dataset)
